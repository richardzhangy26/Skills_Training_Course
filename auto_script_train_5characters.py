import asyncio
import requests
import json
import time
import os
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

class WorkflowTester:
    DEFAULT_PROFILE_KEY = "S2"
    STUDENT_PROFILES = {
        "S1": {
            "label": "S1 æ²‰é»˜å¯¡è¨€çš„å­¦ç”Ÿ",
            "description": "å†…å‘ä¸ä¸»åŠ¨è¡¨è¾¾ï¼Œåªç»™æœ€ç®€çŸ­çš„å›åº”ï¼Œå¸¸ç”¨è¯é‡å¤ã€‚",
            "speech_habit": "å¸¸è¯´â€œå—¯â€â€œå¥½â€â€œä¸çŸ¥é“â€ï¼Œå›ç­”å¤šä¸º 1-2 å¥ç”šè‡³åªæœ‰è¯è¯­ã€‚",
            "style": "è¯­æ°”å…‹åˆ¶ã€ä¿¡æ¯é‡å°‘ï¼Œé™¤éè¢«è¿½é—®ä¸ä¼šå±•å¼€ã€‚",
            "test_goal": "æµ‹è¯• AI æ˜¯å¦èƒ½ä¸»åŠ¨å¼•å¯¼ã€è¿½é—®ï¼Œé¿å…å¯¹è¯ä¸­æ–­ã€‚",
        },
        "S2": {
            "label": "S2 è¯å¤šè·‘é¢˜çš„å­¦ç”Ÿ",
            "description": "å…´å¥‹å¥è°ˆï¼Œå–œæ¬¢åˆ†äº«å„ç§ç»†èŠ‚ï¼Œå¸¸æŠŠè¯é¢˜å¸¦ç¦»å½“å‰é—®é¢˜ã€‚",
            "speech_habit": "å›ç­”å†—é•¿è·³è·ƒï¼Œç»å¸¸å¤¹æ‚ä¸é—®é¢˜å¼±ç›¸å…³çš„ç»å†æˆ–æ„Ÿå—ã€‚",
            "style": "è¯­é€Ÿå¿«ã€æƒ…ç»ªé«˜æ¶¨ï¼Œæƒ³åˆ°ä»€ä¹ˆå°±è¯´ä»€ä¹ˆï¼Œå¾ˆéš¾ä¿æŒä¸­å¿ƒã€‚",
            "test_goal": "æµ‹è¯• AI çš„è¯é¢˜æ”¶æŸèƒ½åŠ›å’Œè€å¿ƒå¼•å¯¼èƒ½åŠ›ã€‚",
        },
        "S3": {
            "label": "S3 é«˜éœ€æ±‚çš„å®Œç¾ä¸»ä¹‰è€…",
            "description": "å¯¹ç­”æ¡ˆæåº¦æŒ‘å‰”ï¼ŒæŒç»­è¿½é—®ç»†èŠ‚å¹¶è¦æ±‚æ›´å¤šç¤ºä¾‹ã€‚",
            "speech_habit": "ä¹ æƒ¯åå¤è¿½é—®â€œè¿˜æœ‰å—â€â€œèƒ½å…·ä½“ç‚¹å—â€ï¼Œä¸æ–­å¼ºè°ƒæ ‡å‡†è¦æ›´é«˜ã€‚",
            "style": "è¯­æ°”è‹›æ±‚ä¸”ä¸¥è°¨ï¼Œæ€»åœ¨å¯»æ‰¾ä¸è¶³ä¹‹å¤„ã€‚",
            "test_goal": "æµ‹è¯• AI çš„æ·±åº¦è§£ç­”èƒ½åŠ›å’Œé¢å¯¹é«˜æ ‡å‡†çš„åº”å¯¹ç­–ç•¥ã€‚",
        },
        "S4": {
            "label": "S4 é€»è¾‘æŒ‘åˆºå‹å­¦ç”Ÿ",
            "description": "å–œæ¬¢æ‰¾ AI çš„çŸ›ç›¾æˆ–æ¼æ´ï¼Œä¸“æ³¨äºè´¨ç–‘å’Œåé©³ã€‚",
            "speech_habit": "ä¹ æƒ¯å…ˆæŒ‡å‡ºä¸åˆç†ç‚¹ï¼Œå†è¦æ±‚ç»™è§£é‡Šï¼Œç”šè‡³æŠ›å‡ºåä¾‹ã€‚",
            "style": "è¯­æ°”çŠ€åˆ©çˆ±è¾©è®ºï¼ŒåŠ¨ä¸åŠ¨å°±è¯´â€œè¿™è¯´ä¸é€šâ€ã€‚",
            "test_goal": "æµ‹è¯• AI çš„é€»è¾‘ä¸€è‡´æ€§ä¸æŠ—è´¨ç–‘èƒ½åŠ›ã€‚"
        },
        "S5": {
            "label": "S5 æƒ…ç»ªåŒ–å­¦ç”Ÿ",
            "description": "æƒ…ç»ªæ³¢åŠ¨å¤§ï¼Œå®¹æ˜“æ²®ä¸§æˆ–ç”Ÿæ°”ï¼Œè¯­è¨€å¤¹æ‚æƒ…ç»ªè¯æ±‡ã€‚",
            "speech_habit": "ä¼šçªç„¶è¡¨è¾¾â€œæˆ‘å¿«å´©æºƒäº†â€â€œå¤ªè®©äººå¤±æœ›â€ç­‰æ„Ÿå—ã€‚",
            "style": "è¯­æ°”å¸¦æƒ…ç»ªè‰²å½©ï¼Œæ—¶è€Œæ¿€åŠ¨æ—¶è€Œä½è½ã€‚",
            "test_goal": "æµ‹è¯• AI çš„æƒ…ç»ªå®‰æŠšä¸æ­£å‘å¼•å¯¼èƒ½åŠ›ã€‚"
        }
    }

    def __init__(self, base_url="https://cloudapi.polymas.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session_id = None
        self.current_step_id = None
        self.task_id = None
        self.dialogue_round = 0
        self.base_path = Path(__file__).resolve().parent
        self.log_root = self.base_path / "log"
        self.run_card_log_path = None
        self.dialogue_log_path = None
        self.log_prefix = None
        self.student_profile_key = None
        self.dialogue_samples_content = None
        self.log_context_path = None
        self.conversation_history = []  # å­˜å‚¨å¯¹è¯å†å²
        
        # ä»ç¯å¢ƒå˜é‡åŠ è½½è®¤è¯ä¿¡æ¯
        load_dotenv()
        
        self.headers = {
            "Content-Type": "application/json",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        }
        
        # æ·»åŠ è®¤è¯ä¿¡æ¯
        authorization = os.getenv("AUTHORIZATION")
        cookie = os.getenv("COOKIE")
        
        if authorization:
            self.headers["Authorization"] = authorization
        
        if cookie:
            self.headers["Cookie"] = cookie
        
        # æ·»åŠ å…¶ä»–å¯é€‰çš„è¯·æ±‚å¤´
        custom_headers = os.getenv("CUSTOM_HEADERS")
        if custom_headers:
            try:
                extra_headers = json.loads(custom_headers)
                self.headers.update(extra_headers)
            except json.JSONDecodeError:
                print("âš ï¸  è­¦å‘Š: CUSTOM_HEADERS æ ¼å¼ä¸æ­£ç¡®ï¼Œå·²å¿½ç•¥")

        # æ¨¡å‹é…ç½®
        self.model_type = os.getenv("MODEL_TYPE", "doubao_sdk")  # doubao_sdk, doubao_post, deepseek_sdk
        self.doubao_client = None
        self.deepseek_client = None
        self.doubao_model = os.getenv("DOUBAO_MODEL", "doubao-seed-1-6-251015")
        self.deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        self.knowledge_base_content = None

        # POST è°ƒç”¨é…ç½®
        self.llm_api_url = os.getenv("LLM_API_URL", "http://llm-service.polymas.com/api/openai/v1/chat/completions")
        self.llm_api_key = os.getenv("LLM_API_KEY", "")
        self.llm_model = os.getenv("LLM_MODEL", "Doubao-1.5-pro-32k")
        self.llm_service_code = os.getenv("LLM_SERVICE_CODE", "SI_Ability")
        self.use_post_api = os.getenv("USE_POST_API", "false").lower() == "true"

        self._initialize_llm_client()

    def _initialize_llm_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        print(f"ğŸ”§ æ¨¡å‹ç±»å‹: {self.model_type}")

        if self.model_type == "doubao_post":
            print(f"   - ä½¿ç”¨ Doubao POST API è°ƒç”¨æ¨¡å¼")
            print(f"   - API URL: {self.llm_api_url}")
            print(f"   - Model: {self.llm_model}")
            print(f"   - Service Code: {self.llm_service_code}")
            if not self.llm_api_key:
                print("âš ï¸  è­¦å‘Š: LLM_API_KEY æœªè®¾ç½®")

        elif self.model_type == "doubao_sdk":
            api_key = os.getenv("ARK_API_KEY")
            base_url = os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")

            if api_key:
                try:
                    self.doubao_client = OpenAI(api_key=api_key, base_url=base_url)
                    print(f"   - ä½¿ç”¨ Doubao OpenAI SDK è°ƒç”¨æ¨¡å¼")
                    print(f"   - Model: {self.doubao_model}")
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: åˆå§‹åŒ– Doubao å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            else:
                print("âš ï¸  è­¦å‘Š: ARK_API_KEY æœªè®¾ç½®")

        elif self.model_type == "deepseek_sdk":
            api_key = os.getenv("DEEPSEEK_API_KEY")

            if api_key:
                try:
                    self.deepseek_client = OpenAI(
                        api_key=api_key,
                        base_url="https://api.deepseek.com"
                    )
                    print(f"   - ä½¿ç”¨ DeepSeek OpenAI SDK è°ƒç”¨æ¨¡å¼")
                    print(f"   - Model: {self.deepseek_model}")
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            else:
                print("âš ï¸  è­¦å‘Š: DEEPSEEK_API_KEY æœªè®¾ç½®")
        else:
            print(f"âš ï¸  è­¦å‘Š: æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")

    def _call_doubao_post(self, messages, temperature=0.85, max_tokens=1000):
        """ä½¿ç”¨ HTTP POST æ–¹å¼è°ƒç”¨ Doubao API"""
        headers = {
            "Content-Type": "application/json",
            "service-code": self.llm_service_code,
        }

        if self.llm_api_key:
            headers["api-key"] = self.llm_api_key

        payload = {
            "model": self.llm_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": 0.95,
            "frequency_penalty": 0.3,
            "presence_penalty": 0.2
        }

        try:
            response = requests.post(
                self.llm_api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
        except requests.exceptions.RequestException as e:
            print(f"âŒ HTTP POST è°ƒç”¨å¤±è´¥: {str(e)}")
            return None
        except (KeyError, IndexError) as e:
            print(f"âŒ è§£æå“åº”å¤±è´¥: {str(e)}")
            return None

    def _prepare_log_files(self, task_id):
        """åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¹¶å†™å…¥å¼€å¤´ä¿¡æ¯"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = self._determine_log_directory(task_id)
        log_dir.mkdir(parents=True, exist_ok=True)
        self.log_prefix = f"task_{task_id}_{timestamp}"
        self.run_card_log_path = log_dir / f"{self.log_prefix}_runcard.txt"
        self.dialogue_log_path = log_dir / f"{self.log_prefix}_dialogue.txt"
        profile_label = self._get_student_profile_info()["label"] if self.student_profile_key else "æœªè®¾ç½®"

        header_lines = [
            f"æ—¥å¿—åˆ›å»ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"task_id: {task_id}",
            f"å­¦ç”Ÿè§’è‰²: {profile_label}"
        ]
        if self.log_context_path:
            header_lines.append(f"å‚è€ƒæ–‡æ¡£: {str(self.log_context_path)}")
        header_lines.append("=" * 60)
        header = "\n".join(header_lines) + "\n"
        for path, title in [
            (self.run_card_log_path, "RunCard ä¿¡æ¯è®°å½•"),
            (self.dialogue_log_path, "å¯¹è¯è®°å½•"),
        ]:
            with open(path, "w", encoding="utf-8") as f:
                f.write(title + "\n")
                f.write(header)

    def _append_log(self, path, text):
        if not path:
            return
        with open(path, "a", encoding="utf-8") as f:
            f.write(text + "\n")

    def _log_run_card(self, step_id, payload, response_data):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_lines = [
            f"[{timestamp}] Step {step_id}",
            f"è¯·æ±‚è½½è·: {json.dumps(payload, ensure_ascii=False)}",
            f"å“åº”å†…å®¹: {json.dumps(response_data, ensure_ascii=False)}",
            "-" * 80,
        ]
        self._append_log(self.run_card_log_path, "\n".join(log_lines))

    def _log_dialogue_entry(self, step_id, user_text=None, ai_text=None, source="chat"):
        if user_text is None and ai_text is None:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        round_info = f" | ç¬¬ {self.dialogue_round} è½®" if self.dialogue_round else ""
        header = f"[{timestamp}] Step {step_id}{round_info} | æ¥æº: {source}"
        lines = [header]
        if user_text:
            lines.append(f"ç”¨æˆ·: {user_text}")
        if ai_text:
            lines.append(f"AI: {ai_text}")
        lines.append("-" * 80)
        self._append_log(self.dialogue_log_path, "\n".join(lines))

    def _get_log_context_parts(self):
        if not self.log_context_path:
            return []

        path = self.log_context_path
        if not isinstance(path, Path):
            path = Path(path)

        try:
            path = path.resolve()
        except Exception:
            pass

        try:
            relative = path.relative_to(self.base_path)
        except ValueError:
            relative = path

        parts = list(relative.parts)
        if not parts:
            return []

        if "skills_training_course" in parts:
            idx = parts.index("skills_training_course")
            parts = parts[idx + 1 :]

        if not parts:
            return []

        trimmed = []
        for i, part in enumerate(parts):
            if i == len(parts) - 1:
                trimmed.append(Path(part).stem)
            else:
                trimmed.append(part)
        return trimmed

    def _determine_log_directory(self, task_id):
        profile_key = self.student_profile_key or "unassigned"
        context_parts = self._get_log_context_parts()
        if context_parts:
            return self.log_root.joinpath(*context_parts, profile_key)
        return self.log_root / f"task_{task_id}" / profile_key

    def _update_log_context(self, new_path):
        if not new_path:
            return

        try:
            path = Path(new_path).expanduser().resolve()
        except Exception:
            path = Path(new_path)

        priority = "skills_training_course" in path.parts
        if priority or not self.log_context_path:
            self.log_context_path = path

    def _get_student_profile_info(self):
        key = self.student_profile_key or self.DEFAULT_PROFILE_KEY
        return self.STUDENT_PROFILES.get(
            key,
            self.STUDENT_PROFILES[self.DEFAULT_PROFILE_KEY]
        )

    def set_student_profile(self, profile_key):
        if profile_key not in self.STUDENT_PROFILES:
            raise ValueError(f"æœªçŸ¥çš„å­¦ç”Ÿè§’è‰²: {profile_key}")
        self.student_profile_key = profile_key
        info = self._get_student_profile_info()
        print(f"\nğŸ“ å·²é€‰æ‹©å­¦ç”Ÿè§’è‰²: {info['label']}")

    def prompt_student_profile(self, allow_multi=False):
        """äº¤äº’å¼é€‰æ‹©å­¦ç”Ÿè§’è‰²ï¼Œå¯é€‰å¤šè§’è‰²"""
        print("\nè¯·é€‰æ‹©å­¦ç”Ÿè§’è‰²ï¼ˆ5 ç§æ€§æ ¼ï¼‰ï¼š")
        options = {}
        for idx, (key, info) in enumerate(self.STUDENT_PROFILES.items(), 1):
            options[str(idx)] = key
            print(f"{idx}. {info['label']} - {info['description']}")

        default_choice = next(
            (num for num, key in options.items() if key == self.DEFAULT_PROFILE_KEY),
            "1"
        )

        tip = "å¯è¾“å…¥å¤šä¸ªç¼–å·å¹¶ç”¨é€—å·åˆ†éš”" if allow_multi else "åªéœ€è¾“å…¥ä¸€ä¸ªç¼–å·"
        prompt_template = (
            f"\nè¯·è¾“å…¥é€‰é¡¹ (1-{len(options)}ï¼Œé»˜è®¤ {default_choice}ï¼Œ{tip}): "
        )

        while True:
            raw_choice = input(prompt_template).strip()
            if not raw_choice:
                raw_choice = default_choice

            selections = [c.strip() for c in raw_choice.split(",") if c.strip()]
            if not selections:
                selections = [default_choice]

            if all(choice in options for choice in selections):
                chosen_keys = []
                for choice in selections:
                    mapped = options[choice]
                    if mapped not in chosen_keys:
                        chosen_keys.append(mapped)

                if not allow_multi:
                    self.set_student_profile(chosen_keys[0])
                    return chosen_keys

                labels = "ï¼Œ".join(self.STUDENT_PROFILES[key]["label"] for key in chosen_keys)
                print(f"\nğŸ¯ å·²é€‰æ‹© {len(chosen_keys)} ä¸ªå­¦ç”Ÿè§’è‰²: {labels}")
                return chosen_keys

            print("âš ï¸  æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

    def _clone_for_parallel(self):
        """å¤åˆ¶å½“å‰å®ä¾‹çš„ä¸Šä¸‹æ–‡ä¾›å¹¶å‘è¿è¡Œä½¿ç”¨"""
        clone = WorkflowTester(self.base_url)
        clone.headers = self.headers.copy()
        clone.model_type = self.model_type
        clone.doubao_model = self.doubao_model
        clone.deepseek_model = self.deepseek_model
        clone.dialogue_samples_content = self.dialogue_samples_content
        clone.knowledge_base_content = self.knowledge_base_content
        clone.log_context_path = self.log_context_path
        # å¤åˆ¶ API é…ç½®
        clone.llm_api_url = self.llm_api_url
        clone.llm_api_key = self.llm_api_key
        clone.llm_model = self.llm_model
        clone.llm_service_code = self.llm_service_code
        clone.conversation_history = []  # æ¯ä¸ªå…‹éš†å®ä¾‹ç‹¬ç«‹çš„å¯¹è¯å†å²
        # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
        clone._initialize_llm_client()
        return clone

    def _run_profile_workflow(self, task_id, profile_key):
        """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œå•ä¸ªå­¦ç”Ÿè§’è‰²çš„ LLM æµç¨‹"""
        runner = self._clone_for_parallel()
        runner.set_student_profile(profile_key)
        runner.run_with_llm(task_id)

    async def run_profiles_concurrently(self, task_id, profile_keys):
        """å¼‚æ­¥å¹¶å‘è¿è¡Œå¤šä¸ªå­¦ç”Ÿè§’è‰²"""
        if not profile_keys:
            print("âš ï¸  æœªé€‰æ‹©å­¦ç”Ÿè§’è‰²ï¼Œæ— æ³•å¹¶å‘è¿è¡Œã€‚")
            return

        print(f"\nğŸš€ æ­£åœ¨å¹¶å‘è¿è¡Œ {len(profile_keys)} ä¸ªå­¦ç”Ÿè§’è‰²...")
        tasks = [
            asyncio.to_thread(self._run_profile_workflow, task_id, profile_key)
            for profile_key in profile_keys
        ]
        await asyncio.gather(*tasks)
        print("\nâœ… æ‰€æœ‰é€‰å®šçš„å­¦ç”Ÿè§’è‰²å·²è¿è¡Œå®Œæˆã€‚")

    def load_student_dialogues(self, md_path):
        """åŠ è½½å­¦ç”Ÿè§’è‰²çš„æ¨¡æ‹Ÿå¯¹è¯ Markdown"""
        try:
            path = Path(md_path)
            if not path.exists():
                print(f"âŒ æ¨¡æ‹Ÿå¯¹è¯æ–‡ä»¶ä¸å­˜åœ¨: {md_path}")
                return False
            self.dialogue_samples_content = path.read_text(encoding="utf-8")
            print(f"âœ… å·²åŠ è½½æ¨¡æ‹Ÿå¯¹è¯: {md_path} (å¤§å°: {len(self.dialogue_samples_content)} å­—ç¬¦)")
            self._update_log_context(path)
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡æ‹Ÿå¯¹è¯å¤±è´¥: {str(e)}")
            return False

    def load_knowledge_base(self, kb_path):
        """åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶"""
        try:
            path = Path(kb_path)
            if not path.exists():
                print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")
                return False

            self.knowledge_base_content = path.read_text(encoding="utf-8")
            print(f"âœ… çŸ¥è¯†åº“å·²åŠ è½½: {kb_path} (å¤§å°: {len(self.knowledge_base_content)} å­—ç¬¦)")
            self._update_log_context(path)
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            return False

    def generate_answer_with_llm(self, question):
        """ä½¿ç”¨ LLM æ¨¡å‹ç”Ÿæˆå›ç­”"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„è°ƒç”¨æ–¹å¼
        if self.model_type == "doubao_sdk" and not self.doubao_client:
            print("âŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        elif self.model_type == "deepseek_sdk" and not self.deepseek_client:
            print("âŒ DeepSeek å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        elif self.model_type == "doubao_post" and not self.llm_api_url:
            print("âŒ POST API URL æœªé…ç½®")
            return None

        try:
            profile_info = self._get_student_profile_info()
            system_prompt = (
                "ä½ æ˜¯ä¸€åèƒ½åŠ›è®­ç»ƒåŠ©æ‰‹ï¼Œéœ€è¦æ¨¡æ‹Ÿå­¦ç”Ÿè§’è‰²è¿›è¡Œå›ç­”ã€‚"
                "æ³¨æ„ï¼šæ€§æ ¼ç‰¹ç‚¹åº”è¯¥è‡ªç„¶èå…¥å¯¹è¯ï¼Œè€Œéç”Ÿç¡¬å¥—ç”¨ï¼Œè¦ä¿æŒå›ç­”çš„çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚"
                "å¦‚æœæœ‰è§’è‰²ç¤ºä¾‹å¯¹è¯ï¼Œè¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™ã€‚"
            )

            sections = [
                "## è§’è‰²è®¾å®š",
                f"å­¦ç”Ÿè§’è‰²: {profile_info['label']}",
                f"è§’è‰²ç‰¹å¾: {profile_info['description']}",
            ]

            if profile_info.get("speech_habit"):
                sections.append(f"è¯´è¯ä¹ æƒ¯: {profile_info['speech_habit']}")

            sections.append(f"è¡¨è¾¾é£æ ¼: {profile_info['style']}")

            if profile_info.get("test_goal"):
                sections.append(f"æµ‹è¯•ç›®çš„: {profile_info['test_goal']}")

            sections.append("")

            # æ·»åŠ é—®é¢˜ç±»å‹è¯†åˆ«
            sections.extend([
                "## é—®é¢˜ç±»å‹è¯†åˆ«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰",
                "å¦‚æœå½“å‰é—®é¢˜å±äºä»¥ä¸‹ç±»å‹ï¼Œè¯·ä¼˜å…ˆç›´æ¥å›ç­”ï¼Œä¸éœ€è¦å¼ºåˆ¶ä½“ç°æ€§æ ¼ç‰¹ç‚¹ï¼š",
                "1. **ç¡®è®¤å¼é—®é¢˜**: å¦‚'ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿè¯·å›å¤æ˜¯æˆ–å¦'ã€'ç¡®è®¤çš„è¯è¯·å›å¤æ˜¯'",
                "   â†’ ç›´æ¥å›ç­”'æ˜¯'ã€'å¥½çš„'ã€'ç¡®è®¤'ç­‰",
                "2. **é€‰æ‹©å¼é—®é¢˜**: å¦‚'ä½ é€‰æ‹©Aè¿˜æ˜¯Bï¼Ÿ'ã€'è¯·é€‰æ‹©1/2/3'",
                "   â†’ ç›´æ¥è¯´å‡ºé€‰é¡¹ï¼Œå¦‚'æˆ‘é€‰æ‹©A'ã€'é€‰1'",
                "3. **è§’è‰²ç¡®è®¤é—®é¢˜**: å¦‚'ä½ æ˜¯å­¦ç”Ÿè¿˜æ˜¯è€å¸ˆï¼Ÿ'",
                "   â†’ ç›´æ¥å›ç­”è§’è‰²ï¼Œå¦‚'å­¦ç”Ÿ'",
                "",
                "**åˆ¤æ–­æ ‡å‡†**: å¦‚æœé—®é¢˜ä¸­åŒ…å«'è¯·å›å¤'ã€'è¯·é€‰æ‹©'ã€'æ˜¯æˆ–å¦'ã€'A/B/C'ç­‰æ˜ç¡®æŒ‡ç¤ºï¼Œåˆ™ä¸ºå°é—­å¼é—®é¢˜ã€‚",
                ""
            ])

            if self.dialogue_samples_content:
                sections.extend([
                    "## è§’è‰²ç¤ºä¾‹å¯¹è¯ (å¦‚æœ‰åŒ¹é…è¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™ï¼Œä¼˜å…ˆçº§æœ€é«˜)",
                    self.dialogue_samples_content,
                    "",
                ])

            if self.knowledge_base_content:
                sections.extend([
                    "## å‚è€ƒçŸ¥è¯†åº“ (å¯ç»“åˆä½¿ç”¨)",
                    self.knowledge_base_content,
                    "",
                ])

            # æ·»åŠ å¯¹è¯å†å²
            if self.conversation_history:
                sections.extend([
                    "## å¯¹è¯å†å²ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰",
                ])
                for i, turn in enumerate(self.conversation_history, 1):
                    sections.append(f"ç¬¬{i}è½®:")
                    sections.append(f"  AIæé—®: {turn['ai']}")
                    sections.append(f"  å­¦ç”Ÿå›ç­”: {turn['student']}")
                sections.append("")

            sections.extend([
                "## å½“å‰é—®é¢˜",
                question,
                "",
                "## è¾“å‡ºè¦æ±‚ï¼ˆæŒ‰ä¼˜å…ˆçº§æ‰§è¡Œï¼‰",
                "**ä¼˜å…ˆçº§1**: ä¼˜å…ˆè¾“å‡ºè§’è‰²ç¤ºä¾‹å¯¹è¯ä¸­çš„å†…å®¹",
                "**ä¼˜å…ˆçº§2**: å¦‚æœæ˜¯å¼€æ”¾å¼é—®é¢˜ï¼Œå†é€‚åº¦èå…¥å­¦ç”Ÿæ€§æ ¼ç‰¹ç‚¹ï¼Œä½†è¦æ³¨æ„ï¼š",
                "   - æ€§æ ¼ç‰¹ç‚¹åº”è¯¥è‡ªç„¶ä½“ç°ï¼Œä¸è¦ç”Ÿç¡¬å¥—ç”¨",
                "   - é¿å…æ¯æ¬¡éƒ½ä½¿ç”¨ç›¸åŒçš„è¯æœ¯ï¼ˆå¦‚ä¸è¦æ€»è¯´'è¿™è¯´ä¸é€š'ã€'ä¸çŸ¥é“'ç­‰ï¼‰",
                "   - ä¿æŒå›ç­”çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ï¼Œå¯ä»¥å¶å°”æ­£å¸¸å›ç­”",
                "**ä¼˜å…ˆçº§3**: å¦‚æœç¤ºä¾‹å¯¹è¯ä¸­æœ‰é«˜åº¦ç›¸å…³çš„å›ç­”ï¼Œå¯ä»¥å‚è€ƒä½†éœ€å˜åŒ–è¡¨è¾¾æ–¹å¼ã€‚",
                "**æ ¼å¼è¦æ±‚**: ä»…è¿”å›å­¦ç”Ÿå›ç­”å†…å®¹ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚",
                ""
            ])

            user_message = "\n".join(sections)

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            # æ ¹æ®é…ç½®é€‰æ‹©è°ƒç”¨æ–¹å¼
            if self.model_type == "doubao_post":
                print("ğŸ”„ ä½¿ç”¨ Doubao POST API è°ƒç”¨...")
                answer = self._call_doubao_post(messages, temperature=0.85, max_tokens=1000)
            elif self.model_type == "doubao_sdk":
                print("ğŸ”„ ä½¿ç”¨ Doubao OpenAI SDK è°ƒç”¨...")
                response = self.doubao_client.chat.completions.create(
                    model=self.doubao_model,
                    messages=messages,
                    temperature=0.85,  # æé«˜æ¸©åº¦å¢åŠ éšæœºæ€§å’Œå¤šæ ·æ€§
                    top_p=0.95,
                    frequency_penalty=0.3,  # é™ä½é‡å¤æ€§
                    presence_penalty=0.2    # é¼“åŠ±ä½¿ç”¨æ–°è¯æ±‡
                )
                answer = response.choices[0].message.content
            elif self.model_type == "deepseek_sdk":
                print("ğŸ”„ ä½¿ç”¨ DeepSeek OpenAI SDK è°ƒç”¨...")
                response = self.deepseek_client.chat.completions.create(
                    model=self.deepseek_model,
                    messages=messages,
                    temperature=0.85,  # æé«˜æ¸©åº¦å¢åŠ éšæœºæ€§å’Œå¤šæ ·æ€§
                    top_p=0.95,
                    frequency_penalty=0.3,  # é™ä½é‡å¤æ€§
                    presence_penalty=0.2    # é¼“åŠ±ä½¿ç”¨æ–°è¯æ±‡
                )
                answer = response.choices[0].message.content
            else:
                print(f"âŒ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")
                return None

            return answer
        except Exception as e:
            print(f"âŒ è°ƒç”¨ {self.model_type} æ¨¡å‹å¤±è´¥: {str(e)}")
            return None

    def test_connection(self):
        """æµ‹è¯•æ¥å£è¿æ¥å’Œè®¤è¯æ˜¯å¦æ­£å¸¸"""
        print("\n" + "="*60)
        print("ğŸ” å¼€å§‹æµ‹è¯•æ¥å£è¿æ¥...")
        print("="*60)
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        print("\n1ï¸âƒ£  æ£€æŸ¥ç¯å¢ƒå˜é‡:")
        auth = os.getenv("AUTHORIZATION")
        cookie = os.getenv("COOKIE")
        
        if not auth and not cookie:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ° AUTHORIZATION æˆ– COOKIE")
            return False
        
        if auth:
            print(f"âœ… AUTHORIZATION: {auth[:20]}...")
        if cookie:
            print(f"âœ… COOKIE: {cookie[:50]}...")
        
        # æµ‹è¯•ç½‘ç»œè¿æ¥
        print("\n2ï¸âƒ£  æµ‹è¯•ç½‘ç»œè¿æ¥:")
        try:
            response = requests.get(self.base_url, timeout=5)
            print(f"âœ… æœåŠ¡å™¨å¯è®¿é—® (çŠ¶æ€ç : {response.status_code})")
            return True
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def query_script_step_list(self, task_id):
        """
        è·å–å·¥ä½œæµçš„æ­¥éª¤åˆ—è¡¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ª stepId
        """
        url = f"{self.base_url}/teacher-course/abilityTrain/queryScriptStepList"
        payload = {
            "trainTaskId": task_id,
            "trainSubType": "ability"
        }
        
        print(f"\n=== è·å–æ­¥éª¤åˆ—è¡¨ ===")
        print(f"è¯·æ±‚URL: {url}")
        # print(f"è¯·æ±‚è½½è·: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        try:
            response = self.session.post(url, json=payload, headers=self.headers, timeout=30)
            result = response.json()
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            # print(f"å“åº”å†…å®¹: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            if result.get("code") == 200 and result.get("success"):
                data = result.get("data") or []
                if data and len(data) > 0:
                    first_step_id = data[2].get("stepId")
                    print(f"\nâœ… è·å–åˆ°ç¬¬ä¸€ä¸ªæ­¥éª¤ID: {first_step_id}")
                    return first_step_id
                else:
                    raise Exception("æ­¥éª¤åˆ—è¡¨ä¸ºç©º")
            else:
                raise Exception(f"è·å–æ­¥éª¤åˆ—è¡¨å¤±è´¥: {result.get('msg')}")
                
        except requests.exceptions.Timeout:
            raise Exception("è¯·æ±‚è¶…æ—¶")
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    
    def run_card(self, task_id, step_id, session_id=None):
        """
        è¿è¡Œå·¥ä½œæµå¡ç‰‡
        """
        url = f"{self.base_url}/ai-tools/trainRun/runCard"
        
        payload = {
            "taskId": task_id,
            "stepId": step_id,
            "sessionId": session_id
        }
        
        # å¦‚æœæœ‰ sessionIdï¼Œæ·»åŠ åˆ°è½½è·ä¸­
        if session_id:
            payload["sessionId"] = session_id
        
        print(f"\n=== è¿è¡Œå¡ç‰‡ (stepId: {step_id}) ===")
        print(f"è¯·æ±‚URL: {url}")
        print(f"è¯·æ±‚è½½è·: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        try:
            response = self.session.post(url, json=payload, headers=self.headers, timeout=30)
            result = response.json()
            self._log_run_card(step_id, payload, result)
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            # print(f"å“åº”å†…å®¹: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            if result.get("code") == 200 and result.get("success"):
                data = result.get("data") or {}
                self.session_id = data.get("sessionId")
                self.current_step_id = step_id
                
                self.question_text = data.get("text")
                need_skip = data.get("needSkipStep", False)
                
                if self.question_text:
                    print(f"\nğŸ“ AI è¯´: {self.question_text}")
                    self._log_dialogue_entry(step_id, ai_text=self.question_text, source="runCard")
                
                return result
            else:
                print("è®­ç»ƒå®Œæˆ")
                return result
                
        except requests.exceptions.Timeout:
            raise Exception("è¯·æ±‚è¶…æ—¶")
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    
    def chat(self, user_input, step_id=None):
        """
        å‘é€ç”¨æˆ·å›ç­”
        """
        url = f"{self.base_url}/ai-tools/trainRun/chat"
        
        if step_id is None:
            step_id = self.current_step_id
        
        payload = {
            "taskId": self.task_id,
            "stepId": step_id,
            "text": user_input,
            "sessionId": self.session_id
        }
        
        print(f"\n=== å‘é€ç”¨æˆ·å›ç­” ===")
        print(f"ğŸ‘¤ ç”¨æˆ·è¯´: {user_input}")
        # print(f"è¯·æ±‚è½½è·: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        try:
            response = self.session.post(url, json=payload, headers=self.headers, timeout=30)
            result = response.json()
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            # print(f"å“åº”å†…å®¹: {json.dumps(result, indent=2, ensure_ascii=False)}")
            
            if result.get("code") == 200 and result.get("success"):
                data = result.get("data") or {}
                next_step_id = data.get("nextStepId")
                need_skip = data.get("needSkipStep", False)
                ai_text = data.get("text")
                self.dialogue_round += 1
                self._log_dialogue_entry(step_id, user_text=user_input, ai_text=ai_text, source="chat")

                if ai_text:
                    print(f"\nğŸ“ AI è¯´: {ai_text}")
                    # æ›´æ–°å½“å‰é—®é¢˜æ–‡æœ¬ï¼Œä¾›ä¸‹ä¸€è½®ç”Ÿæˆå›ç­”ä½¿ç”¨
                    self.question_text = ai_text

                # å…³é”®é€»è¾‘ï¼šå¦‚æœ needSkipStep=true ä¸” nextStepId ä¸ä¸ºç©ºï¼Œéœ€è¦è°ƒç”¨ runCard
                if need_skip and next_step_id:
                    print(f"\nâ­ï¸  éœ€è¦è·³è½¬åˆ°ä¸‹ä¸€æ­¥éª¤: {next_step_id}")
                    print("è‡ªåŠ¨è°ƒç”¨ runCard...")
                    self.current_step_id=next_step_id
                    return self.run_card(self.task_id, next_step_id, self.session_id)
                else:
                    return result
            else:
                raise Exception(f"å‘é€æ¶ˆæ¯å¤±è´¥: {result.get('msg')}")
                
        except requests.exceptions.Timeout:
            raise Exception("è¯·æ±‚è¶…æ—¶")
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    
    def start_workflow(self, task_id):
        """
        å¯åŠ¨å·¥ä½œæµ
        1. è·å–ç¬¬ä¸€ä¸ª stepId
        2. è°ƒç”¨ runCard å¼€å§‹ç¬¬ä¸€æ­¥
        """
        print("\n" + "="*60)
        print("ğŸš€ å¯åŠ¨å·¥ä½œæµ")
        print("="*60)
        
        self.task_id = task_id
        self.dialogue_round = 0
        self.conversation_history = []  # é‡ç½®å¯¹è¯å†å²
        self._prepare_log_files(task_id)
        
        # 1. è·å–ç¬¬ä¸€ä¸ªæ­¥éª¤ID
        first_step_id = self.query_script_step_list(task_id)

        # 2. è¿è¡Œç¬¬ä¸€ä¸ªå¡ç‰‡
        result = self.run_card(task_id, first_step_id)
        
        return result
    
    def run_interactive(self, task_id):
        """
        äº¤äº’å¼è¿è¡Œå·¥ä½œæµ
        """
        try:
            # å¯åŠ¨å·¥ä½œæµ
            self.start_workflow(task_id)
            
            round_num = 1
            
            # å¾ªç¯å¯¹è¯
            while True:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€æ­¥
                if self.current_step_id is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼æ²¡æœ‰æ›´å¤šæ­¥éª¤äº†ã€‚")
                    break
                
                print("\n" + "="*60)
                print(f"ğŸ’¬ ç¬¬ {round_num} è½®å¯¹è¯")
                print("="*60)
                
                user_answer = input("è¯·è¾“å…¥ä½ çš„å›ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰: ").strip()
                
                if user_answer.lower() == 'quit':
                    print("ğŸ‘‹ ç”¨æˆ·ä¸»åŠ¨é€€å‡º")
                    break
                
                if not user_answer:
                    print("âš ï¸  å›ç­”ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                    continue
                
                # å‘é€ç”¨æˆ·å›ç­”
                result = self.chat(user_answer)
                
                # æ£€æŸ¥è¿”å›ç»“æœä¸­çš„ nextStepId
                data = result.get("data") or {}
                if data.get("nextStepId") is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼")
                    break
                
                round_num += 1
                time.sleep(0.5)  # ç¨å¾®å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def run_auto(self, task_id, user_answers):
        """
        è‡ªåŠ¨åŒ–è¿è¡Œå·¥ä½œæµï¼ˆä½¿ç”¨é¢„è®¾ç­”æ¡ˆï¼‰
        """
        try:
            # å¯åŠ¨å·¥ä½œæµ
            self.start_workflow(task_id)

            # å¾ªç¯å›ç­”é—®é¢˜
            for i, answer in enumerate(user_answers, 1):
                if self.current_step_id is None:
                    print("\nâœ… å·¥ä½œæµå·²ç»“æŸ")
                    break

                print(f"\n--- ç¬¬ {i} è½®å¯¹è¯ ---")
                time.sleep(1)

                result = self.chat(answer)

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                data = result.get("data") or {}
                if data.get("nextStepId") is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼")
                    break

            print("\n" + "="*60)
            print("ğŸ‰ å·¥ä½œæµæµ‹è¯•ç»“æŸ")
            print("="*60)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

    def run_with_llm(self, task_id):
        """
        ä½¿ç”¨ LLM æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå›ç­”å¹¶è¿è¡Œå·¥ä½œæµ
        """
        # æ£€æŸ¥å®¢æˆ·ç«¯åˆå§‹åŒ–
        if self.model_type == "doubao_sdk" and not self.doubao_client:
            print("\nâŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ ARK_API_KEY ç¯å¢ƒå˜é‡")
            return
        elif self.model_type == "deepseek_sdk" and not self.deepseek_client:
            print("\nâŒ DeepSeek å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            return
        elif self.model_type == "doubao_post" and not self.llm_api_url:
            print("\nâŒ POST API URL æœªé…ç½®")
            return

        if not self.student_profile_key:
            default_label = self.STUDENT_PROFILES[self.DEFAULT_PROFILE_KEY]["label"]
            print(f"\nâš ï¸  æœªæŒ‡å®šå­¦ç”Ÿè§’è‰²ï¼Œé»˜è®¤ä½¿ç”¨ '{default_label}'ã€‚")
            self.student_profile_key = self.DEFAULT_PROFILE_KEY

        try:
            # å¯åŠ¨å·¥ä½œæµ
            self.start_workflow(task_id)

            round_num = 1

            # å¾ªç¯å¯¹è¯
            while True:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€æ­¥
                if self.current_step_id is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼æ²¡æœ‰æ›´å¤šæ­¥éª¤äº†ã€‚")
                    break

                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
                if round_num > 50:
                    print(f"\nâš ï¸  è­¦å‘Šï¼šå·²è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ•°ï¼ˆ{round_num}è½®ï¼‰ï¼Œè‡ªåŠ¨é€€å‡ºé˜²æ­¢æ— é™å¾ªç¯")
                    break

                print("\n" + "="*60)
                print(f"ğŸ¤– ç¬¬ {round_num} è½®å¯¹è¯ï¼ˆ{self.model_type} è‡ªä¸»å›ç­”ï¼‰")
                print("="*60)

                # ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
                print(f"\nğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”...")
                generated_answer = self.generate_answer_with_llm(self.question_text)

                if not generated_answer:
                    print("âŒ æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè·³è¿‡æ­¤è½®")
                    break

                print(f"\nğŸ¤– {self.model_type} ç”Ÿæˆçš„å›ç­”: {generated_answer}")

                # ä¿å­˜å½“å‰è½®å¯¹è¯åˆ°å†å²
                self.conversation_history.append({
                    "ai": self.question_text,
                    "student": generated_answer
                })

                # å‘é€ç”Ÿæˆçš„å›ç­”
                result = self.chat(generated_answer)

                # æ£€æŸ¥è¿”å›ç»“æœï¼Œå¦‚æœ text ä¸º null ä¸” nextStepId ä¸º nullï¼Œä»£è¡¨è¾“å‡ºç»“æŸ
                data = result.get("data") or {}
                if data.get("text") is None and data.get("nextStepId") is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼")
                    break

                round_num += 1
                time.sleep(1)  # ç¨å¾®å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«

            print("\n" + "="*60)
            print("ğŸ‰ å·¥ä½œæµæµ‹è¯•ç»“æŸ")
            print("="*60)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()


# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("="*60)
    print("ğŸ“‹ å¯¹è¯å·¥ä½œæµè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…· v2.0")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = WorkflowTester()
    
    # æµ‹è¯•è¿æ¥
    if not tester.test_connection():
        print("\nâŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·å…ˆè§£å†³é—®é¢˜")
        exit(1)
    
    # è·å– task_id
    task_id = os.getenv("TASK_ID")
    if not task_id:
        task_id = input("\nè¯·è¾“å…¥ task_id: ").strip()
        if not task_id:
            print("âŒ task_id ä¸èƒ½ä¸ºç©º")
            exit(1)
    
    print(f"\nä½¿ç”¨ task_id: {task_id}")
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ–¹å¼ï¼š")
    print("1. äº¤äº’å¼è¿è¡Œï¼ˆæ¨èï¼‰")
    print("2. è‡ªåŠ¨åŒ–è¿è¡Œï¼ˆéœ€è¦é¢„è®¾ç­”æ¡ˆï¼‰")
    print("3. å¤§æ¨¡å‹è‡ªä¸»é€‰æ‹©å›ç­”ï¼ˆDoubao è‡ªåŠ¨ç”Ÿæˆç­”æ¡ˆï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()

    if choice == "1":
        tester.run_interactive(task_id)

    elif choice == "2":
        print("\næç¤º: è¯·å…ˆåœ¨ä»£ç ä¸­é…ç½® user_answers åˆ—è¡¨")
        user_answers = [
            "è¿™æ˜¯ç¬¬ä¸€ä¸ªç­”æ¡ˆ",
            "è¿™æ˜¯ç¬¬äºŒä¸ªç­”æ¡ˆ",
            "è¿™æ˜¯ç¬¬ä¸‰ä¸ªç­”æ¡ˆ"
        ]
        tester.run_auto(task_id, user_answers)

    elif choice == "3":
        print("\nğŸ¤– ä½¿ç”¨ LLM æ¨¡å‹è‡ªä¸»å›ç­”æ¨¡å¼")

        # æ¨¡å‹é€‰æ‹©
        print("\nè¯·é€‰æ‹© LLM æ¨¡å‹ï¼š")
        print("1. Doubao (OpenAI SDK)")
        print("2. Doubao (POST API)")
        print("3. DeepSeek (OpenAI SDK)")

        model_choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3ï¼Œé»˜è®¤ 1): ").strip()
        if model_choice == "2":
            tester.model_type = "doubao_post"
        elif model_choice == "3":
            tester.model_type = "deepseek_sdk"
        else:
            tester.model_type = "doubao_sdk"

        # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
        tester._initialize_llm_client()

        multi_mode = input(
            "\næ˜¯å¦éœ€è¦åŒæ—¶è¿è¡Œå¤šä¸ªå­¦ç”Ÿè§’è‰²ï¼Ÿ(y/nï¼Œé»˜è®¤ n): "
        ).strip().lower() == "y"
        selected_profiles = tester.prompt_student_profile(allow_multi=multi_mode)

        print("\nå¯é€‰: æ˜¯å¦æä¾›å­¦ç”Ÿè§’è‰²æ¨¡æ‹Ÿå¯¹è¯ Markdownï¼Ÿ")
        use_dialogue_md = input("æ˜¯å¦åŠ è½½æ¨¡æ‹Ÿå¯¹è¯ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_dialogue_md == "y":
            dialogue_path = input("\nè¯·è¾“å…¥ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if dialogue_path:
                tester.load_student_dialogues(dialogue_path)
            else:
                print("âš ï¸  æœªæä¾›è·¯å¾„ï¼Œè·³è¿‡åŠ è½½æ¨¡æ‹Ÿå¯¹è¯")

        print("\nå¯é€‰: æ˜¯å¦ä½¿ç”¨å¤–æ¥çŸ¥è¯†åº“ï¼Ÿ")
        use_kb = input("æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_kb == "y":
            kb_path = input("\nè¯·è¾“å…¥çŸ¥è¯†åº“ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if kb_path:
                if not tester.load_knowledge_base(kb_path):
                    print("âš ï¸  çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼Œå°†ä»¥é€šç”¨æ¨¡å¼è¿è¡Œ")
            else:
                print("âš ï¸  æœªæä¾›çŸ¥è¯†åº“è·¯å¾„ï¼Œè·³è¿‡åŠ è½½")

        print("\nå¼€å§‹å·¥ä½œæµ...")
        if multi_mode:
            asyncio.run(tester.run_profiles_concurrently(task_id, selected_profiles))
        else:
            tester.run_with_llm(task_id)

    else:
        print("âŒ æ— æ•ˆé€‰é¡¹")