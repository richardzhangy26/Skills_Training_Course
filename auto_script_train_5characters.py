import asyncio
import requests
import json
import time
import os
from datetime import datetime
from pathlib import Path
from openai import OpenAI
from workflow_tester_base import WorkflowTesterBase

class WorkflowTester(WorkflowTesterBase):
    DEFAULT_PROFILE_KEY = "S2"
    PROFILE_LABEL_FIELD_NAME = "å­¦ç”Ÿè§’è‰²"
    PROFILE_SELECT_TITLE = "å­¦ç”Ÿè§’è‰²"
    DEFAULT_STUDENT_PROFILES = {
        "S1": {
            "label": "S1 æ²‰é»˜å¯¡è¨€çš„å­¦ç”Ÿ",
            "description": "å†…å‘ä¸ä¸»åŠ¨è¡¨è¾¾ï¼Œåªç»™æœ€ç®€çŸ­çš„å›åº”ï¼Œå¸¸ç”¨è¯é‡å¤ã€‚",
            "speech_habit": "å¸¸è¯´â€œå—¯â€â€œå¥½â€â€œä¸çŸ¥é“â€ï¼Œå›ç­”å¤šä¸º 1-2 å¥ç”šè‡³åªæœ‰è¯è¯­ã€‚",
            "style": "è¯­æ°”å…‹åˆ¶ã€ä¿¡æ¯é‡å°‘ï¼Œé™¤éè¢«è¿½é—®ä¸ä¼šå±•å¼€ã€‚",
            "test_goal": "æµ‹è¯• AI æ˜¯å¦èƒ½ä¸»åŠ¨å¼•å¯¼ã€è¿½é—®ï¼Œé¿å…å¯¹è¯ä¸­æ–­ã€‚",
        },
        "S2": {
            "label": "S2 è¯å¤šè·‘é¢˜çš„å­¦ç”Ÿ",
            "description": "å…´å¥‹å¥è°ˆï¼Œå–œæ¬¢åˆ†äº«å„ç§ç»†èŠ‚ï¼Œå¸¸æŠŠè¯é¢˜å¸¦ç¦»å½“å‰é—®é¢˜ã€‚",
            "speech_habit": "å›ç­”å†—é•¿è·³è·ƒï¼Œç»å¸¸å¤¹æ‚ä¸é—®é¢˜å¼±ç›¸å…³çš„ç»å†æˆ–æ„Ÿå—ã€‚",
            "style": "è¯­é€Ÿå¿«ã€æƒ…ç»ªé«˜æ¶¨ï¼Œæƒ³åˆ°ä»€ä¹ˆå°±è¯´ä»€ä¹ˆï¼Œå¾ˆéš¾ä¿æŒä¸­å¿ƒã€‚",
            "test_goal": "æµ‹è¯• AI çš„è¯é¢˜æ”¶æŸèƒ½åŠ›å’Œè€å¿ƒå¼•å¯¼èƒ½åŠ›ã€‚",
        },
        "S3": {
            "label": "S3 é«˜éœ€æ±‚çš„å®Œç¾ä¸»ä¹‰è€…",
            "description": "å¯¹ç­”æ¡ˆæåº¦æŒ‘å‰”ï¼ŒæŒç»­è¿½é—®ç»†èŠ‚å¹¶è¦æ±‚æ›´å¤šç¤ºä¾‹ã€‚",
            "speech_habit": "ä¹ æƒ¯åå¤è¿½é—®â€œè¿˜æœ‰å—â€â€œèƒ½å…·ä½“ç‚¹å—â€ï¼Œä¸æ–­å¼ºè°ƒæ ‡å‡†è¦æ›´é«˜ã€‚",
            "style": "è¯­æ°”è‹›æ±‚ä¸”ä¸¥è°¨ï¼Œæ€»åœ¨å¯»æ‰¾ä¸è¶³ä¹‹å¤„ã€‚",
            "test_goal": "æµ‹è¯• AI çš„æ·±åº¦è§£ç­”èƒ½åŠ›å’Œé¢å¯¹é«˜æ ‡å‡†çš„åº”å¯¹ç­–ç•¥ã€‚",
        },
        "S4": {
            "label": "S4 é€»è¾‘æŒ‘åˆºå‹å­¦ç”Ÿ",
            "description": "å–œæ¬¢æ‰¾ AI çš„çŸ›ç›¾æˆ–æ¼æ´ï¼Œä¸“æ³¨äºè´¨ç–‘å’Œåé©³ã€‚",
            "speech_habit": "ä¹ æƒ¯å…ˆæŒ‡å‡ºä¸åˆç†ç‚¹ï¼Œå†è¦æ±‚ç»™è§£é‡Šï¼Œç”šè‡³æŠ›å‡ºåä¾‹ã€‚",
            "style": "è¯­æ°”çŠ€åˆ©çˆ±è¾©è®ºï¼ŒåŠ¨ä¸åŠ¨å°±è¯´â€œè¿™è¯´ä¸é€šâ€ã€‚",
            "test_goal": "æµ‹è¯• AI çš„é€»è¾‘ä¸€è‡´æ€§ä¸æŠ—è´¨ç–‘èƒ½åŠ›ã€‚"
        },
        "S5": {
            "label": "S5 æƒ…ç»ªåŒ–å­¦ç”Ÿ",
            "description": "æƒ…ç»ªæ³¢åŠ¨å¤§ï¼Œå®¹æ˜“æ²®ä¸§æˆ–ç”Ÿæ°”ï¼Œè¯­è¨€å¤¹æ‚æƒ…ç»ªè¯æ±‡ã€‚",
            "speech_habit": "ä¼šçªç„¶è¡¨è¾¾â€œæˆ‘å¿«å´©æºƒäº†â€â€œå¤ªè®©äººå¤±æœ›â€ç­‰æ„Ÿå—ã€‚",
            "style": "è¯­æ°”å¸¦æƒ…ç»ªè‰²å½©ï¼Œæ—¶è€Œæ¿€åŠ¨æ—¶è€Œä½è½ã€‚",
            "test_goal": "æµ‹è¯• AI çš„æƒ…ç»ªå®‰æŠšä¸æ­£å‘å¼•å¯¼èƒ½åŠ›ã€‚"
        }
    }

    def __init__(self, base_url="https://cloudapi.polymas.com"):
        super().__init__(base_url)

        # åŠ è½½å­¦ç”Ÿæ€§æ ¼é…ç½®
        self.student_profiles = self._load_student_profiles()

        # æ¨¡å‹é…ç½®
        self.model_type = os.getenv("MODEL_TYPE", "doubao_post")  # doubao_sdk, doubao_post, deepseek_sdk
        self.doubao_client = None
        self.deepseek_client = None
        self.deepseek_client = None
        self.doubao_model = os.getenv("DOUBAO_MODEL", "doubao-seed-1-6-251015")
        self.deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

        # POST è°ƒç”¨é…ç½®
        self.llm_api_url = os.getenv(
            "LLM_API_URL",
            "http://llm-service.polymas.com/api/openai/v1/chat/completions",
        )
        self.llm_api_key = os.getenv("LLM_API_KEY", "")
        self.llm_model = os.getenv("LLM_MODEL", "Doubao-1.5-pro-32k")
        self.llm_service_code = os.getenv("LLM_SERVICE_CODE", "SI_Ability")
        self.use_post_api = os.getenv("USE_POST_API", "false").lower() == "true"

        self._initialize_llm_client()

    def _load_student_profiles(self):
        """åŠ è½½å­¦ç”Ÿæ€§æ ¼é…ç½®æ–‡ä»¶"""
        config_paths = [
            Path.cwd() / "student_profiles.custom.json",
            self.base_path / "student_profiles.json"
        ]

        for config_path in config_paths:
            if config_path.exists():
                try:
                    return self._load_config_file(config_path)
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {str(e)}")

        # ä½¿ç”¨å†…ç½®é»˜è®¤é…ç½®
        print("âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤é…ç½®")
        return self.DEFAULT_STUDENT_PROFILES

    def _load_config_file(self, config_path):
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # éªŒè¯é…ç½®æ–‡ä»¶ç»“æ„
            if "profiles" not in config:
                raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘ 'profiles' å­—æ®µ")

            profiles = config["profiles"]
            validated_profiles = {}

            # éªŒè¯å¹¶åˆå¹¶æ¯ä¸ªæ€§æ ¼é…ç½®
            for profile_key in ["S1", "S2", "S3", "S4", "S5"]:
                if profile_key in profiles:
                    user_profile = profiles[profile_key]
                    default_profile = self.DEFAULT_STUDENT_PROFILES.get(profile_key, {})

                    # åˆå¹¶é…ç½®ï¼šç”¨æˆ·é…ç½®è¦†ç›–é»˜è®¤é…ç½®
                    merged_profile = default_profile.copy()
                    merged_profile.update(user_profile)

                    # ç¡®ä¿å¿…å¡«å­—æ®µå­˜åœ¨
                    required_fields = ["label", "description", "speech_habit", "style", "test_goal"]
                    for field in required_fields:
                        if field not in merged_profile:
                            merged_profile[field] = default_profile.get(field, "")

                    # æ·»åŠ  enabled å­—æ®µï¼ˆé»˜è®¤ä¸º trueï¼‰
                    if "enabled" not in merged_profile:
                        merged_profile["enabled"] = True

                    validated_profiles[profile_key] = merged_profile
                else:
                    # ä½¿ç”¨é»˜è®¤é…ç½®
                    validated_profiles[profile_key] = self.DEFAULT_STUDENT_PROFILES[profile_key]

            print(f"âœ… å·²åŠ è½½å­¦ç”Ÿæ€§æ ¼é…ç½®: {config_path}")
            return validated_profiles

        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}")
        except Exception as e:
            raise ValueError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _initialize_llm_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        print(f"ğŸ”§ æ¨¡å‹ç±»å‹: {self.model_type}")

        if self.model_type == "doubao_post":
            print(f"   - ä½¿ç”¨ Doubao POST API è°ƒç”¨æ¨¡å¼")
            print(f"   - API URL: {self.llm_api_url}")
            print(f"   - Model: {self.llm_model}")
            print(f"   - Service Code: {self.llm_service_code}")
            if not self.llm_api_key:
                print("âš ï¸  è­¦å‘Š: LLM_API_KEY æœªè®¾ç½®")

        elif self.model_type == "doubao_sdk":
            api_key = os.getenv("ARK_API_KEY")
            base_url = os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")

            if api_key:
                try:
                    self.doubao_client = OpenAI(api_key=api_key, base_url=base_url)
                    print(f"   - ä½¿ç”¨ Doubao OpenAI SDK è°ƒç”¨æ¨¡å¼")
                    print(f"   - Model: {self.doubao_model}")
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: åˆå§‹åŒ– Doubao å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            else:
                print("âš ï¸  è­¦å‘Š: ARK_API_KEY æœªè®¾ç½®")

        elif self.model_type == "deepseek_sdk":
            api_key = os.getenv("DEEPSEEK_API_KEY")

            if api_key:
                try:
                    self.deepseek_client = OpenAI(
                        api_key=api_key,
                        base_url="https://api.deepseek.com"
                    )
                    print(f"   - ä½¿ç”¨ DeepSeek OpenAI SDK è°ƒç”¨æ¨¡å¼")
                    print(f"   - Model: {self.deepseek_model}")
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            else:
                print("âš ï¸  è­¦å‘Š: DEEPSEEK_API_KEY æœªè®¾ç½®")
        else:
            print(f"âš ï¸  è­¦å‘Š: æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")

    def _call_doubao_post(self, messages, temperature=0.85, max_tokens=1000):
        """ä½¿ç”¨ HTTP POST æ–¹å¼è°ƒç”¨ Doubao API"""
        headers = {
            "Content-Type": "application/json",
            "service-code": self.llm_service_code,
        }

        if self.llm_api_key:
            headers["api-key"] = self.llm_api_key

        payload = {
            "model": self.llm_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": 0.95,
            "frequency_penalty": 0.3,
            "presence_penalty": 0.2
        }

        try:
            response = requests.post(
                self.llm_api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
        except requests.exceptions.RequestException as e:
            print(f"âŒ HTTP POST è°ƒç”¨å¤±è´¥: {str(e)}")
            return None
        except (KeyError, IndexError) as e:
            print(f"âŒ è§£æå“åº”å¤±è´¥: {str(e)}")
            return None

    def _get_current_model_name(self):
        """è·å–å½“å‰æ¨¡å‹çš„æ˜¾ç¤ºåç§°"""
        if self.model_type == "doubao_post":
            return self.llm_model
        elif self.model_type == "doubao_sdk":
            return self.doubao_model
        elif self.model_type == "deepseek_sdk":
            return self.deepseek_model
        return "unknown"

    def _clone_for_parallel(self):
        """å¤åˆ¶å½“å‰å®ä¾‹çš„ä¸Šä¸‹æ–‡ä¾›å¹¶å‘è¿è¡Œä½¿ç”¨"""
        clone = WorkflowTester(self.base_url)
        clone.headers = self.headers.copy()
        clone.model_type = self.model_type
        clone.doubao_model = self.doubao_model
        clone.deepseek_model = self.deepseek_model
        clone.dialogue_samples_content = self.dialogue_samples_content
        clone.knowledge_base_content = self.knowledge_base_content
        clone.log_context_path = self.log_context_path
        # å¤åˆ¶ API é…ç½®
        clone.llm_api_url = self.llm_api_url
        clone.llm_api_key = self.llm_api_key
        clone.llm_model = self.llm_model
        clone.llm_service_code = self.llm_service_code
        clone.conversation_history = []  # æ¯ä¸ªå…‹éš†å®ä¾‹ç‹¬ç«‹çš„å¯¹è¯å†å²
        clone.step_name_mapping = self.step_name_mapping.copy()  # å¤åˆ¶æ­¥éª¤åç§°æ˜ å°„
        # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
        clone._initialize_llm_client()
        return clone

    def _run_profile_workflow(self, task_id, profile_key):
        """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œå•ä¸ªå­¦ç”Ÿè§’è‰²çš„ LLM æµç¨‹"""
        runner = self._clone_for_parallel()
        runner.set_student_profile(profile_key)
        runner.run_with_llm(task_id)

    async def run_profiles_concurrently(self, task_id, profile_keys):
        """å¼‚æ­¥å¹¶å‘è¿è¡Œå¤šä¸ªå­¦ç”Ÿè§’è‰²"""
        if not profile_keys:
            print("âš ï¸  æœªé€‰æ‹©å­¦ç”Ÿè§’è‰²ï¼Œæ— æ³•å¹¶å‘è¿è¡Œã€‚")
            return

        print(f"\nğŸš€ æ­£åœ¨å¹¶å‘è¿è¡Œ {len(profile_keys)} ä¸ªå­¦ç”Ÿè§’è‰²...")
        tasks = [
            asyncio.to_thread(self._run_profile_workflow, task_id, profile_key)
            for profile_key in profile_keys
        ]
        await asyncio.gather(*tasks)
        print("\nâœ… æ‰€æœ‰é€‰å®šçš„å­¦ç”Ÿè§’è‰²å·²è¿è¡Œå®Œæˆã€‚")

    def generate_answer_with_llm(self, question):
        """ä½¿ç”¨ LLM æ¨¡å‹ç”Ÿæˆå›ç­”"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„è°ƒç”¨æ–¹å¼
        if self.model_type == "doubao_sdk" and not self.doubao_client:
            print("âŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        elif self.model_type == "deepseek_sdk" and not self.deepseek_client:
            print("âŒ DeepSeek å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        elif self.model_type == "doubao_post" and not self.llm_api_url:
            print("âŒ POST API URL æœªé…ç½®")
            return None

        try:
            profile_info = self._get_student_profile_info()
            system_prompt = (
                "ä½ æ˜¯ä¸€åèƒ½åŠ›è®­ç»ƒåŠ©æ‰‹ï¼Œéœ€è¦æ¨¡æ‹Ÿå­¦ç”Ÿè§’è‰²è¿›è¡Œå›ç­”ã€‚"
                "æ³¨æ„ï¼šæ€§æ ¼ç‰¹ç‚¹åº”è¯¥è‡ªç„¶èå…¥å¯¹è¯ï¼Œè€Œéç”Ÿç¡¬å¥—ç”¨ï¼Œè¦ä¿æŒå›ç­”çš„çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚"
                "å¦‚æœæœ‰è§’è‰²ç¤ºä¾‹å¯¹è¯ï¼Œè¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™ã€‚"
            )

            sections = [
                "## è§’è‰²è®¾å®š",
                f"å­¦ç”Ÿè§’è‰²: {profile_info['label']}",
                f"è§’è‰²ç‰¹å¾: {profile_info['description']}",
            ]

            if profile_info.get("speech_habit"):
                sections.append(f"è¯´è¯ä¹ æƒ¯: {profile_info['speech_habit']}")

            sections.append(f"è¡¨è¾¾é£æ ¼: {profile_info['style']}")

            if profile_info.get("test_goal"):
                sections.append(f"æµ‹è¯•ç›®çš„: {profile_info['test_goal']}")

            sections.append("")

            # æ·»åŠ é—®é¢˜ç±»å‹è¯†åˆ«
            sections.extend([
                "## é—®é¢˜ç±»å‹è¯†åˆ«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰",
                "å¦‚æœå½“å‰é—®é¢˜å±äºä»¥ä¸‹ç±»å‹ï¼Œè¯·ä¼˜å…ˆç›´æ¥å›ç­”ï¼Œä¸éœ€è¦å¼ºåˆ¶ä½“ç°æ€§æ ¼ç‰¹ç‚¹ï¼š",
                "1. **ç¡®è®¤å¼é—®é¢˜**: å¦‚'ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿè¯·å›å¤æ˜¯æˆ–å¦'ã€'ç¡®è®¤çš„è¯è¯·å›å¤æ˜¯'",
                "   â†’ ç›´æ¥å›ç­”'æ˜¯'ã€'å¥½çš„'ã€'ç¡®è®¤'ç­‰",
                "2. **é€‰æ‹©å¼é—®é¢˜**: å¦‚'ä½ é€‰æ‹©Aè¿˜æ˜¯Bï¼Ÿ'ã€'è¯·é€‰æ‹©1/2/3'",
                "   â†’ ç›´æ¥è¯´å‡ºé€‰é¡¹ï¼Œå¦‚'æˆ‘é€‰æ‹©A'ã€'é€‰1'",
                "3. **è§’è‰²ç¡®è®¤é—®é¢˜**: å¦‚'ä½ æ˜¯å­¦ç”Ÿè¿˜æ˜¯è€å¸ˆï¼Ÿ'",
                "   â†’ ç›´æ¥å›ç­”è§’è‰²ï¼Œå¦‚'å­¦ç”Ÿ'",
                "",
                "**åˆ¤æ–­æ ‡å‡†**: å¦‚æœé—®é¢˜ä¸­åŒ…å«'è¯·å›å¤'ã€'è¯·é€‰æ‹©'ã€'æ˜¯æˆ–å¦'ã€'A/B/C'ç­‰æ˜ç¡®æŒ‡ç¤ºï¼Œåˆ™ä¸ºå°é—­å¼é—®é¢˜ã€‚",
                ""
            ])

            if self.dialogue_samples_content:
                sections.extend([
                    "## è§’è‰²ç¤ºä¾‹å¯¹è¯ (å¦‚æœ‰åŒ¹é…è¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™ï¼Œä¼˜å…ˆçº§æœ€é«˜)",
                    self.dialogue_samples_content,
                    "",
                ])

            if self.knowledge_base_content:
                sections.extend([
                    "## å‚è€ƒçŸ¥è¯†åº“ (å¯ç»“åˆä½¿ç”¨)",
                    self.knowledge_base_content,
                    "",
                ])

            # æ·»åŠ å¯¹è¯å†å²
            if self.conversation_history:
                sections.extend([
                    "## å¯¹è¯å†å²ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰",
                ])
                for i, turn in enumerate(self.conversation_history, 1):
                    sections.append(f"ç¬¬{i}è½®:")
                    sections.append(f"  AIæé—®: {turn['ai']}")
                    sections.append(f"  å­¦ç”Ÿå›ç­”: {turn['student']}")
                sections.append("")

            sections.extend([
                "## å½“å‰é—®é¢˜",
                question,
                "",
                "## è¾“å‡ºè¦æ±‚ï¼ˆæŒ‰ä¼˜å…ˆçº§æ‰§è¡Œï¼‰",
                "**ä¼˜å…ˆçº§1**: ä¼˜å…ˆè¾“å‡ºè§’è‰²ç¤ºä¾‹å¯¹è¯ä¸­çš„å†…å®¹",
                "**ä¼˜å…ˆçº§2**: å¦‚æœæ˜¯å¼€æ”¾å¼é—®é¢˜ï¼Œå†é€‚åº¦èå…¥å­¦ç”Ÿæ€§æ ¼ç‰¹ç‚¹ï¼Œä½†è¦æ³¨æ„ï¼š",
                "   - æ€§æ ¼ç‰¹ç‚¹åº”è¯¥è‡ªç„¶ä½“ç°ï¼Œä¸è¦ç”Ÿç¡¬å¥—ç”¨",
                "   - é¿å…æ¯æ¬¡éƒ½ä½¿ç”¨ç›¸åŒçš„è¯æœ¯ï¼ˆå¦‚ä¸è¦æ€»è¯´'è¿™è¯´ä¸é€š'ã€'ä¸çŸ¥é“'ç­‰ï¼‰",
                "   - ä¿æŒå›ç­”çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ï¼Œå¯ä»¥å¶å°”æ­£å¸¸å›ç­”",
                "**ä¼˜å…ˆçº§3**: å¦‚æœç¤ºä¾‹å¯¹è¯ä¸­æœ‰é«˜åº¦ç›¸å…³çš„å›ç­”ï¼Œå¯ä»¥å‚è€ƒä½†éœ€å˜åŒ–è¡¨è¾¾æ–¹å¼ã€‚",
                "**æ ¼å¼è¦æ±‚**: ä»…è¿”å›å­¦ç”Ÿå›ç­”å†…å®¹ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚",
                ""
            ])

            user_message = "\n".join(sections)

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            # æ ¹æ®é…ç½®é€‰æ‹©è°ƒç”¨æ–¹å¼
            if self.model_type == "doubao_post":
                print("ğŸ”„ ä½¿ç”¨ Doubao POST API è°ƒç”¨...")
                answer = self._call_doubao_post(messages, temperature=0.85, max_tokens=1000)
            elif self.model_type == "doubao_sdk":
                print("ğŸ”„ ä½¿ç”¨ Doubao OpenAI SDK è°ƒç”¨...")
                response = self.doubao_client.chat.completions.create(
                    model=self.doubao_model,
                    messages=messages,
                    temperature=0.85,  # æé«˜æ¸©åº¦å¢åŠ éšæœºæ€§å’Œå¤šæ ·æ€§
                    top_p=0.95,
                    frequency_penalty=0.3,  # é™ä½é‡å¤æ€§
                    presence_penalty=0.2    # é¼“åŠ±ä½¿ç”¨æ–°è¯æ±‡
                )
                answer = response.choices[0].message.content
            elif self.model_type == "deepseek_sdk":
                print("ğŸ”„ ä½¿ç”¨ DeepSeek OpenAI SDK è°ƒç”¨...")
                response = self.deepseek_client.chat.completions.create(
                    model=self.deepseek_model,
                    messages=messages,
                    temperature=0.85,  # æé«˜æ¸©åº¦å¢åŠ éšæœºæ€§å’Œå¤šæ ·æ€§
                    top_p=0.95,
                    frequency_penalty=0.3,  # é™ä½é‡å¤æ€§
                    presence_penalty=0.2    # é¼“åŠ±ä½¿ç”¨æ–°è¯æ±‡
                )
                answer = response.choices[0].message.content
            else:
                print(f"âŒ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")
                return None

            return answer
        except Exception as e:
            print(f"âŒ è°ƒç”¨ {self.model_type} æ¨¡å‹å¤±è´¥: {str(e)}")
            print(f"âŒ è°ƒç”¨ {self.model_type} æ¨¡å‹å¤±è´¥: {str(e)}")
            return None

    def run_with_llm(self, task_id):
        """
        ä½¿ç”¨ LLM æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå›ç­”å¹¶è¿è¡Œå·¥ä½œæµ
        """
        # æ£€æŸ¥å®¢æˆ·ç«¯åˆå§‹åŒ–
        if self.model_type == "doubao_sdk" and not self.doubao_client:
            print("\nâŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ ARK_API_KEY ç¯å¢ƒå˜é‡")
            return
        elif self.model_type == "deepseek_sdk" and not self.deepseek_client:
            print("\nâŒ DeepSeek å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            return
        elif self.model_type == "doubao_post" and not self.llm_api_url:
            print("\nâŒ POST API URL æœªé…ç½®")
            return

        if not self.student_profile_key:
            default_label = self.student_profiles[self.DEFAULT_PROFILE_KEY]["label"]
            print(f"\nâš ï¸  æœªæŒ‡å®šå­¦ç”Ÿè§’è‰²ï¼Œé»˜è®¤ä½¿ç”¨ '{default_label}'ã€‚")
            self.student_profile_key = self.DEFAULT_PROFILE_KEY

        try:
            # å¯åŠ¨å·¥ä½œæµ
            self.start_workflow(task_id)

            round_num = 1

            # å¾ªç¯å¯¹è¯
            while True:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€æ­¥
                if self.current_step_id is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼æ²¡æœ‰æ›´å¤šæ­¥éª¤äº†ã€‚")
                    break

                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
                if round_num > 50:
                    print(f"\nâš ï¸  è­¦å‘Šï¼šå·²è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ•°ï¼ˆ{round_num}è½®ï¼‰ï¼Œè‡ªåŠ¨é€€å‡ºé˜²æ­¢æ— é™å¾ªç¯")
                    break

                print("\n" + "="*60)
                print(f"ğŸ¤– ç¬¬ {round_num} è½®å¯¹è¯ï¼ˆ{self.model_type} è‡ªä¸»å›ç­”ï¼‰")
                print("="*60)

                # ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
                print(f"\nğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”...")
                generated_answer = self.generate_answer_with_llm(self.question_text)

                if not generated_answer:
                    print("âŒ æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè·³è¿‡æ­¤è½®")
                    break

                print(f"\nğŸ¤– {self.model_type} ç”Ÿæˆçš„å›ç­”: {generated_answer}")

                # ä¿å­˜å½“å‰è½®å¯¹è¯åˆ°å†å²
                self.conversation_history.append({
                    "ai": self.question_text,
                    "student": generated_answer
                })

                # å‘é€ç”Ÿæˆçš„å›ç­”
                result = self.chat(generated_answer)

                # æ£€æŸ¥è¿”å›ç»“æœï¼Œå¦‚æœ text ä¸º null ä¸” nextStepId ä¸º nullï¼Œä»£è¡¨è¾“å‡ºç»“æŸ
                # æ£€æŸ¥è¿”å›ç»“æœï¼Œå¦‚æœ text ä¸º null ä¸” nextStepId ä¸º nullï¼Œä»£è¡¨è¾“å‡ºç»“æŸ
                data = result.get("data") or {}
                if data.get("text") is None and data.get("nextStepId") is None:
                if data.get("text") is None and data.get("nextStepId") is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼")
                    break

                round_num += 1
                time.sleep(1)  # ç¨å¾®å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«

            self._finalize_workflow()

            print("\n" + "="*60)
            print("ğŸ‰ å·¥ä½œæµæµ‹è¯•ç»“æŸ")
            print("="*60)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()


# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("="*60)
    print("ğŸ“‹ å¯¹è¯å·¥ä½œæµè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…· v2.0")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = WorkflowTester()
    
    # æµ‹è¯•è¿æ¥
    if not tester.test_connection():
        print("\nâŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·å…ˆè§£å†³é—®é¢˜")
        exit(1)
    
    # è·å– task_id
    task_id = os.getenv("TASK_ID")
    if not task_id:
        task_id = input("\nè¯·è¾“å…¥ task_id: ").strip()
        if not task_id:
            print("âŒ task_id ä¸èƒ½ä¸ºç©º")
            exit(1)
    
    print(f"\nä½¿ç”¨ task_id: {task_id}")

    # é€‰æ‹©æ—¥å¿—æ ¼å¼
    tester.log_format = tester._get_log_format_preference()

    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ–¹å¼ï¼š")
    print("1. äº¤äº’å¼è¿è¡Œï¼ˆæ¨èï¼‰")
    print("2. è‡ªåŠ¨åŒ–è¿è¡Œï¼ˆéœ€è¦é¢„è®¾ç­”æ¡ˆï¼‰")
    print("3. å¤§æ¨¡å‹è‡ªä¸»é€‰æ‹©å›ç­”ï¼ˆDoubao è‡ªåŠ¨ç”Ÿæˆç­”æ¡ˆï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()

    if choice == "1":
        tester.run_interactive(task_id)

    elif choice == "2":
        print("\næç¤º: è¯·å…ˆåœ¨ä»£ç ä¸­é…ç½® user_answers åˆ—è¡¨")
        user_answers = [
            "è¿™æ˜¯ç¬¬ä¸€ä¸ªç­”æ¡ˆ",
            "è¿™æ˜¯ç¬¬äºŒä¸ªç­”æ¡ˆ",
            "è¿™æ˜¯ç¬¬ä¸‰ä¸ªç­”æ¡ˆ"
        ]
        tester.run_auto(task_id, user_answers)

    elif choice == "3":
        print("\nğŸ¤– ä½¿ç”¨ LLM æ¨¡å‹è‡ªä¸»å›ç­”æ¨¡å¼")

        # æ¨¡å‹é€‰æ‹©
        print("\nè¯·é€‰æ‹© LLM æ¨¡å‹ï¼š")
        print("1. Doubao (OpenAI SDK)")
        print("2. Doubao (POST API)")
        print("3. DeepSeek (OpenAI SDK)")

        model_choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3ï¼Œé»˜è®¤ 1): ").strip()
        if model_choice == "2":
            tester.model_type = "doubao_post"
        elif model_choice == "3":
            tester.model_type = "deepseek_sdk"
        else:
            tester.model_type = "doubao_sdk"

        # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
        tester._initialize_llm_client()

        multi_mode = input(
            "\næ˜¯å¦éœ€è¦åŒæ—¶è¿è¡Œå¤šä¸ªå­¦ç”Ÿè§’è‰²ï¼Ÿ(y/nï¼Œé»˜è®¤ n): "
        ).strip().lower() == "y"
        selected_profiles = tester.prompt_student_profile(allow_multi=multi_mode)

        print("\nå¯é€‰: æ˜¯å¦æä¾›å­¦ç”Ÿè§’è‰²æ¨¡æ‹Ÿå¯¹è¯ Markdownï¼Ÿ")
        use_dialogue_md = input("æ˜¯å¦åŠ è½½æ¨¡æ‹Ÿå¯¹è¯ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_dialogue_md == "y":
            dialogue_path = input("\nè¯·è¾“å…¥ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if dialogue_path:
                tester.load_student_dialogues(dialogue_path)
            else:
                print("âš ï¸  æœªæä¾›è·¯å¾„ï¼Œè·³è¿‡åŠ è½½æ¨¡æ‹Ÿå¯¹è¯")

        print("\nå¯é€‰: æ˜¯å¦ä½¿ç”¨å¤–æ¥çŸ¥è¯†åº“ï¼Ÿ")
        use_kb = input("æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_kb == "y":
            kb_path = input("\nè¯·è¾“å…¥çŸ¥è¯†åº“ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if kb_path:
                if not tester.load_knowledge_base(kb_path):
                    print("âš ï¸  çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼Œå°†ä»¥é€šç”¨æ¨¡å¼è¿è¡Œ")
            else:
                print("âš ï¸  æœªæä¾›çŸ¥è¯†åº“è·¯å¾„ï¼Œè·³è¿‡åŠ è½½")

        print("\nå¼€å§‹å·¥ä½œæµ...")
        if multi_mode:
            asyncio.run(tester.run_profiles_concurrently(task_id, selected_profiles))
        else:
            tester.run_with_llm(task_id)

    else:
        print("âŒ æ— æ•ˆé€‰é¡¹")
